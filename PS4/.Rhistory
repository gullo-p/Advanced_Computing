install.packages("devtools")
install.packages("dplyr")
install.packages("Rmarkdown")
install.packages("rmarkdown")
?sample
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
install.packages("rmvtnorm")
install.packages("rmvtnorm", dependencies=TRUE)
setRepositories()
install.packages("mvtnorm", dependencies = TRUE)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
library(mvtnorm)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
mixtureProb, replace = TRUE)
rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
mixtureProb, replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC1
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
library(dplyr)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
prob = rep(1/10, 10), replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
drawsC1
whichGaussianC2
meansC1
library(mvtnorm)
library(dplyr)
# 2 class 2 dimensional mixture of Gaussians
genGaussMix <- function(noObs = c(100, 100),
noGaussians = 10,
mixtureProb = rep(1/noGaussians, noGaussians),
seed = 2222) {
set.seed(seed)
# producing means of our bivariate Gaussians
meansC1 <- rmvnorm(noGaussians, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(noGaussians, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), noObs[1],
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
# combining and labeling
dataset <- data.frame(rbind(drawsC1, drawsC2),
label = c(rep("C1", noObs[1]), rep("C2", noObs[2])),
y = c(rep(0, noObs[1]), rep(1, noObs[2])),
stringsAsFactors = FALSE)
return(dataset)
}
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
# generate some data
dataset <- genGaussMix()
str(dataset)
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
plot2dClasses
dataset <- genGaussMix()
str(dataset)
plot2dClasses(dataset)
install.packages("ggplot2")
library(ggplot2)
plot2dClasses(dataset)
genGaussMix <- function(noObs = c(100, 100),
noGaussians = 10,
mixtureProb = rep(1/noGaussians, noGaussians),
seed = 1111) {
set.seed(seed)
# producing means of our bivariate Gaussians
meansC1 <- rmvnorm(noGaussians, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(noGaussians, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), noObs[1],
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
# combining and labeling
dataset <- data.frame(rbind(drawsC1, drawsC2),
label = c(rep("C1", noObs[1]), rep("C2", noObs[2])),
y = c(rep(0, noObs[1]), rep(1, noObs[2])),
stringsAsFactors = FALSE)
return(dataset)
}
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
# generate some data
dataset <- genGaussMix()
str(dataset)
plot2dClasses(dataset)
k <- 1  # odd number
p <- 2  # Manhattan (1), Euclidean (2) or Chebyshev (Inf)
# Compute the distance between each point and all others
# according to the similarity measure
noObs <- nrow(dataset)
distMatrix <- matrix(NA, noObs, noObs)
View(dataset)
features <- as.matrix(dataset[ ,1:2])
View(features)
View(features)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- features[obs,]
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2, byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
View(probeExpanded)
neighbors <- apply(distMatrix, 2, order) %>% t()
# first entry for the first point should be the closest neighbor
neighbors[1,1] == 1
## [1] TRUE
# the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(dataset[neighbors[obs, 1:k], "y"])
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
table(predictedClasses, dataset[,"y"])
##
## predictedClasses   0   1
##                0 100   0
##                1   0 100
mean(predictedClasses == dataset[,"y"])
?apply
View(neighbors)
neighbors <- apply(distMatrix, 1, order) %>% t()
View(neighbors)
all(diag(distMatrix) == 0)
View(distMatrix)
distMatrix
View(neighbors)
View(probeExpanded)
View(neighbors)
neighbors[1,1] == 1
View(neighbors)
neigbors[2,1]
neighbors[2,1]
View(neighbors)
View(dataset)
View(dataset)
table(predictedClasses, dataset[,"y"])
mean(predictedClasses == dataset[,"y"])
install.packages("assert_that")
install.packages("assertthat")
install.packages("assertthat")
install.packages("assertthat")
kNN <- function(X, y, memory = NULL,
k = 1, p = 2, type="train") {
# test the inputs
library(assertthat)
not_empty(X); not_empty(y);
if (type == "train") {
assert_that(nrow(X) == length(y))
}
is.string(type); assert_that(type %in% c("train", "predict"))
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(X) &
nrow(memory) == length(y))
}
# Compute the distance between each point and all others
noObs <- nrow(X)
# if we are making predictions on the test set based on the memory,
# we compute distances between each test observation and observations
# in our memory
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(X -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(X - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(y[neighbors[1:k, obs]])
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
# examine the performance, available only if training
if (type == "train") {
errorCount <- table(predictedClasses, y)
accuracy <- mean(predictedClasses == y)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount))
}
?apply
?new.env
install.packages("class")
MNIST_train <- read.csv("/Users/guglielmo/Desktop/BGSE/winter_term/Adv_Computing/15D012_Advanced_Computational_Methods/datasets/MNIST/MNIST_training.csv", header = FALSE, sep =",")
MNIST_test <- read.csv("/Users/guglielmo/Desktop/BGSE/winter_term/Adv_Computing/15D012_Advanced_Computational_Methods/datasets/MNIST/MNIST_test.csv", header = FALSE, sep =",")
#select the right working directory which has kNN.R function
setwd("/Users/guglielmo/Desktop/BGSE/winter_term/Adv_Computing/Advanced_Computing/PS4") #change the path to the directory PS4 of the github repo
#source the function needed
source("kNN.R")
library(class)
### OPTIMIZATION OF k and p:
## Idea: split the training data in 75% training set and 25% test set and perform
#cross-validation, thus identifying the best k (fixing for e.g. p <- 2).
## After having chosen the best k we can optimize for p using the same technique.
k <- c(1,3,5,7,9,11,15,17,23,25,35,45,55,83,101,151);
p <- 2
?knn
#initialize a matrix that computes the error of the test for each value of k and each sample
errorTest <- matrix(NA, nrow = length(k), ncol = 5)
bound <- floor((nrow(MNIST_train)/4)*3)         #define % of training and test set
df <- MNIST_train[sample(nrow(MNIST_train)), ]           #sample rows
df.train <- df[1:bound, 2:257]                       #get training set
df.test <- df[(bound+1):nrow(df), 2:257]
df.label <- df[1:bound, 1]
df.labtest <- df[(bound+1):nrow(df), 1]
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 1, p = 1, type = "predict", control = FALSE)$predLabels
error2 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 1, p = 2, type = "predict", control = TRUE)$predLabels
error3 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 1, p = 1, type = "predict", control = FALSE)
View(kNN)
error2 <-  mean(a$predLabels!=df.labtest)
distMatrix <- a$distMatrix
neighbors <- a$neighbors
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 1, p = 2, type = "predict", control = TRUE)$predLabels
error3 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 1, p = Inf, type = "predict", control = TRUE)$predLabels
error4 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 3, p = 1, type = "predict", control = FALSE)
error2 <-  mean(a$predLabels!=df.labtest)
distMatrix <- a$distMatrix
neighbors <- a$neighbors
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 3, p = 2, type = "predict", control = TRUE)$predLabels
error3 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 3, p = Inf, type = "predict", control = TRUE)$predLabels
error4 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 11, p = 1, type = "predict", control = FALSE)
error2 <-  mean(a$predLabels!=df.labtest)
distMatrix <- a$distMatrix
neighbors <- a$neighbors
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 3, p = 2, type = "predict", control = TRUE)$predLabels
error3 <-  mean(a!=df.labtest)
a <- kNN(features = df.train,
labels = df.label,
memory = df.test,
k = 3, p = Inf, type = "predict", control = TRUE)$predLabels
error4 <-  mean(a!=df.labtest)
?knn
result <- knn(train = MNIST_train[,2:257], test = MNIST_test[,1:256], cl = MNIST_train[,1], k =1)
result <- as.data.frame(result)
?write.csv
write.csv(result, file = "MNIST_predictions.csv", sep = "", row.names = FALSE)
write.csv(result, file = "MNIST_predictions.csv", quote = FALSE, row.names = FALSE)
